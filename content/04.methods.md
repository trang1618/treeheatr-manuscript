## Methods

Conditional decision trees [@doi:10.1198/106186006X133933] are nonparametric models performing recursive binary partitioning with well-defined theoretical background.
Conditional trees support unbiased selection among covariates and avoid overfitting problems, producing competitive prediction accuracy [@doi:10.1198/106186006X133933].
*treeheatr* utilizes the *ggparty* R package to compute the conditional tree for a classification or regression problem (indirectly via the *partykit* R package) along with its edge and node information.

While *ggparty* assumes fixed leaf node widths, *treeheatr* employs a flexible node layout to accommodate the different number of samples shown in the heatmap at each leaf node.
This new node layout structure supports various leaf node widths, prevents crossings of different tree branches, and generalizes as the trees grow in size.
This new layout weighs the *x*-coordinate of the parent node according to the level of the child nodes in order to avoid branch crossing.
This relative weight can be adjusted with the `lev_fac` parameter in `heat_tree()`.
`lev_fac = 1` sets the parent node's *x*-coordinate perfectly in the middle of those of its child nodes.
The default `lev_fac = 1.3` seems to provide [optimal node layout](https://trang1618.github.io/treeheatr/articles/explore.html#smart-node-layout) independent of the tree size.
The user can define a customized layout for a specific set of nodes and combine that layout with the automatic layout for the remaining nodes.

By default, *treeheatr* automatically performs clustering when organizing the heatmap.
To order the samples, clustering is run on samples within each leaf node of all features and outcome label, unless `clust_target = FALSE`.
*treeheatr* uses the `daisy()` function in the [*cluster*](https://cran.r-project.org/package=cluster) R package with the Gower metric [@doi:10.2307/2528823] to compute the dissimilarity matrix of a dataset that may have both continuous and nominal categorical feature types.
Then, the samples are optimally reordered using the [*seriation*](https://cran.r-project.org/package=cluster/) package [@doi:10.18637/jss.v025.i03].
We note that, while there is no definitive guideline for proper weighting of features of different types, the goal of the clustering step is to reduce the amount of stochasticity in the heatmap and not to make precise inference about each cluster.
Therefore, *treeheatr* does not draw dendrograms and allows for the inclusion of outcome labels in clustering the samples.

In a visualization, it is difficult to strike the balance between enhancing understanding and overloading information.
We believe showing a heatmap at the leaf node space provides additional information of the data in an elegant way that is not overwhelming and may even simplify the model's interpretation. 
We left it for the user to decide what type of information to be displayed at the inner nodes via different *geom* objects (e.g., `geom_node_plot`, `geom_edge_label`, etc.) in the *ggparty* package.
For example, one may choose to show the [distribution](https://github.com/martin-borkovec/ggparty/wiki/1-Motivating-Example) of the features and how they split the samples at these decision nodes, or each feature's corresponding Bonferroni-adjusted *P* values computed in the conditional tree algorithm [@doi:10.1198/106186006X133933].

Striving for simplicity, *treeheatr* utilizes direct labeling to avoid unnecessary legends.
For example, in classification, the leaf node labels have colors corresponding with different classes, e.g., purple for Deceased and green for Survived in the COVID-19 dataset (Fig. @fig:example).
As for feature values, by default, the color scale ranges from 0 to 1 and indicates the relative value of a sample compared to the rest of the group on each feature.
Linking the color values of a particular feature to the corresponding edge labels can reveal additional information that is not available with the decision tree alone.

In addition to the main dataset, the user can supply to `heat_tree()` a validation dataset via the `data_test` argument.
As a result, `heat_tree()` will train the conditional tree on the original training dataset, draw the decision tree-heatmap on the testing dataset, and, if desired, print next to the tree [its performance](https://trang1618.github.io/treeheatr/articles/explore.html#apply-the-learned-tree-on-externalholdouttestvalidation-dataset) on the test set according to specified metrics (e.g., balanced accuracy for classification or root mean squared error for regression problem).
Furthermore, if the default conditional tree is not optimal, one can use the `custom_tree` argument to [manually define](https://trang1618.github.io/treeheatr/articles/explore.html#youre-the-warren-beatty-of-your-heat_tree) their own tree of class `party` or `partynode`.

The integration of heatmap nicely complements the current techniques of visualizing decision trees.
Node purity, a metric measuring the tree's performance, can be visualized from the distribution of true outcome labels at each leaf node in the first row.
Comparing these values with the leaf node label gives a visual estimate of how accurate the tree predictions are.
Further, without specifically choosing two features to show in a 2-D scatter plot, we can infer correlation structures among features in the heatmap.
The additional clustering may also reveal sub-structures within a leaf node.
