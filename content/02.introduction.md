## Introduction

Decision tree models comprise a set of machine learning algorithms widely used for predicting an outcome from a set of predictors or features.
For specific problems, a single decision tree can provide predictions at desirable accuracy while remaining easy to understand and interpret [@doi:10.1038/s42256-020-0180-7].
These models are also important building blocks of more complex tree-based structures such as random forests and gradient boosted trees.

The simplicity of decision tree models allows for clear visualizations that can be incorporated with rich additional information such as the feature space.
However, existing software frequently treats all nodes in a decision tree similarly, leaving limited options for improving information presentation at the leaf nodes.
Specifically, the R library [*rpart.plot*](https://cran.r-project.org/web/packages/rpart.plot/rpart.plot.pdf) displays at each node its characteristics including the number of observations falling in that node, the proportion of those observations in each class, and the node's majority vote.
Despite being potentially helpful, these statistics may not immediately convey important information about the tree such as its overall performance.
Function `visTree()` from the R package [*visNetwork*](http://datastorm-open.github.io/visNetwork/tree.html) draws trees that are aesthetically pleasing but lack general information about the data and are difficult to interpret.
The state-of-the-art Python's [*dtreeviz*](https://github.com/parrt/dtreeviz) produces decision trees with detailed histograms at inner nodes but still draw pie chart of different classes at leaf nodes.

[*ggparty*](https://cran.r-project.org/web/packages/ggparty/vignettes/ggparty-graphic-partying.html) is a flexible R package that allows the user to have full control of the representation of each node.
However, this library fixes the leaf node widths, which limits its ability to show more collective visualizations.
We have developed the *treeheatr* package to incorporate the functionality of *ggparty* but also utilize the leaf node space to show the data as a heatmap, a popular visualization that uncovers clusters of samples and features in a dataset [@doi:10.1198/tas.2009.0033, @doi:10.1093/bioinformatics/btx657].
In addition to the clusters, a heatmap also displays a useful general view of the dataset, e.g., how large it is or whether it contains any outliers.
Integrated with a decision tree, the samples are clustered based on not only a distance metric calculated from the features but also which leaf nodes of the tree they belong to.
After simple installation, the user can apply *treeheatr* on their classification or regression problem with a single function:
```r
heat_tree(data, task = 'classification', target_lab = 'Outcome')
```
This one line of code above will generate the conditional inference tree, perform clustering, and produce a decision tree-heatmap as a *ggplot* object that can be viewed in RStudio's viewer pane, saved to a graphic file, or embedded in an RMarkdown document.
This example assumes a classification problem, but one can also apply *treeheatr* on a regression problem by changing the `task` argument.

This article is organized as follows.
In Section 2, we show an example *treeheatr* application by employing its functions on a real-world clinical dataset from a study of diabetes mellitus in a high risk population of Pima Indians [@pmcid:PMC2245318].
In Section 3, we describe in detail the important functions and corresponding arguments in *treeheatr*.
We demonstrate the flexibility the user has in tweaking these arguments to enhance understanding of the tree-based models applied on their dataset.
Finally, we discuss general guidelines for creating effective decision tree-heatmap visualization.

