## A simple example

This example visualizes the conditional inference tree model built to predict whether or not a patient has diabetes from a dataset provided by the National Institute of Diabetes and Digestive and Kidney Diseases [@pmcid:PMC2245318].
This dataset of 768 female patients at least 21 years old of Pima Indian heritage near Phoenix, Arizona was downloaded from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database) and has eight features: age, number of pregnancies, plasma glucose concentration, diastolic blood pressure, skin fold thickness, 2-hour serum insulin, body mass index (BMI) and diabetes pedigree function.
Detailed descriptions of these variables and data source can be found on the Kaggle page.

The following lines of code compute and visualize the conditional decision tree along with the heatmap containing features that are important for constructing this model (Fig. @fig:example):

```r
heat_tree(
  data = diabetes,
  target_lab = 'Diabetes status',
  label_map = c(`0` = 'Negative', `1` = 'Positive')
)
```

The `heat_tree()` function takes a data frame, a character string indicating the column name associated with the outcome/phenotype (e.g., Diabetes status) and other optional arguments such as the mapping of the outcome label. 

![A decision tree-heatmap for predicting whether an individual has diabetes.](images/diabetes.png){#fig:example}

In the decision tree, the leaf nodes are labeled based on their majority votes and colored to correlate with the true outcome.
On the left branches, while these samples are predicted to not have diabetes by majority voting, the leaf nodes have different purity.
These seemingly non-beneficial splits present an opportunity to teach machine learning novices the different measures of node impurity such as the Gini index or cross-entropy [@isbn:978-0387848570].

In the heatmap, each (very thin) column is a sample, and each row represents a feature or the outcome.
For a specific feature, the color indicates the relative value of a sample compared to the rest of the group on that feature; higher values are associated with lighter colors.
Within the heatmap, similar color patterns between age and the number of pregnancies indicate a correlation between these two features, which is expected.

Together, the tree and heatmap give us an approximation of the proportion of samples per leaf and the model's confidence in its classification of samples in each leaf.
We also observe that glucose level is the first determining factors in predicting diabetes status.
When this value is above 127 but not larger than 154 (observations with light green glucose value), BMI helps further distinguish the group with diabetes from the other.
Here, if we focus on the BMI â‰¤ 29.9 branch, we notice that the corresponding BMI colors range from purple to dark blue (< 0.5), illustrating that the individuals in this branch have BMI lower than the median of the group.
This connection is immediate with the two components visualized together but would not have been possible with the tree model alone.
In summary, the tree and heatmap integration provides a comprehensive view of the data along with key characteristics of the decision tree.