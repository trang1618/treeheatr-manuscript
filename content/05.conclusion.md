## Conclusion

In this paper, we presented a new type of integrated visualization of decision trees and heatmaps, which provides a comprehensive data overview as well as model interpretation.
We demonstrated that this integration uncovers meaningful patterns among the predictive features and highlights the important elements of decision trees including feature splits and several leaf node characteristics such as prediction value, impurity and number of leaf samples.
Its detailed vignette makes *treeheatr* a useful teaching tool to enhance students' understanding of this fundamental model before diving into more complex tree-based machine learning methods.
This package has been released as open source software.

*treeheatr* is scalable to large datasets.
For example, `heat_tree()` runtime on the [waveform dataset](http://archive.ics.uci.edu/ml/datasets/waveform+database+generator+(version+2)) with 5000 observations and 40 features was approximately 80 seconds on a machine with a 2.2 GHz Intel Core i7 processor and 8GB of RAM.
However, as with other visualization tools, the tree's interpretation becomes more difficult as the feature space expands.
Thus, for high dimensional datasets, it's potentially beneficial to perform feature selection to reduce the number of features or random sampling to reduce the number of observations prior to plotting the tree.

Future work on *treeheatr* includes enhancements such as support for left-to-right orientation and heatmap visualization of a holdout set and highlighting the tree branches that point to a specific holdout sample.
Simple data preprocess beyond scaling and normalizing the features may be beneficial for handling outliers and missing data points, which might result in more robust models and informative visualizations.